{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 04 - Introduction to Machine Learning concepts\n",
    "Machine learning is a field of computer science that uses statistical techniques to give computer systems the ability to \"learn\" from data (e.g., gradually improve performance at a given task) without being explicitly programmed.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/3/34/Unraveling_AI_Complexity_-_A_Comparative_View_of_AI%2C_Machine_Learning%2C_Deep_Learning%2C_and_Generative_AI.png\" width=\"500\">\n",
    "\n",
    "## Overview:\n",
    "1. The Machine Learning Workflow\n",
    "2. Types of Machine Learning\n",
    "3. Supervised Learning\n",
    "    * 3.1. Regression\n",
    "    * 3.2. Classification\n",
    "4. Unsupervised Learning\n",
    "    * 4.1. Clustering\n",
    "    * 4.2. Dimensionality Reduction\n",
    "5. Reinforcement Learning\n",
    "6. The bias-variance tradeoff\n",
    "7. Hyperparameter tuning methods\n",
    "    * 7.1. Grid search\n",
    "    * 7.2. Random search\n",
    "    * 7.3. Bayesian optimization\n",
    "    * 7.4. Genetic algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Machine Learning Workflow\n",
    "\n",
    "1. **Define the problem:** define the business problem and the desired outcomes.\n",
    "2. **Collect the data**\n",
    "3. **Prepare the data**: build the data processing pipeline for cleaning, transforming, and feature engineering that the model requires.\n",
    "4. **Choose a model:** select an appropiate algorithm to produce the desired results.\n",
    "5. **Train the model:** train a model on a portion less than 80% of the data.\n",
    "6. **Evaluate the model:** evaluate the model on the remaining 20% of the data. Choose the evaluation metric that best fits the problem.\n",
    "7. **Repeat steps 4-6 until the model is satisfactory**\n",
    "8. **Deploy or productionize the model:** deploy the model to production.\n",
    "\n",
    "<img src=\"https://www.gatevidyalay.com/wp-content/uploads/2019/12/Machine-Learning-Workflow.png\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Branches of Machine Learning algorithms\n",
    "\n",
    "1. Supervised Learning\n",
    "2. Unsupervised Learning\n",
    "3. Reinforcement Learning\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1358/1*8wU0hfUY3UK_D8Y7tbIyFQ.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Supervised Learning\n",
    "Supervised learning is a type of machine learning where the algorithm learns from a labeled dataset, which consists of input features and their corresponding target variables or labels. These labels are the “response variable,” “target variable,” or “output variable” – in other words, the thing you are trying to predict.\n",
    "\n",
    "There are two types of supervised modeling that we will focus on:\n",
    "* **Regression**\n",
    "* **Classification**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Regression\n",
    "Regression is a specific type of supervised learning where the goal is to predict continuous numerical values.\n",
    "\n",
    "Some common regression algorithms include:\n",
    "* **Linear regression**\n",
    "* **Polynomial regression**\n",
    "* **K-nearest neighbors**\n",
    "* **Extreme Gradient Boosting (XGBoost)**\n",
    "* **Support Vector Machines (SVM)**\n",
    "* **Random Forest** and **Decision Trees**, although they are mostly used for classification.\n",
    "* **Neural Networks**\n",
    "\n",
    "**Linear regression** and **polynomial regression** are common examples of regression algorithms that are used to model relationships between variables in a continuous setting.\n",
    "\n",
    "Here are some common **regression model evaluation metrics**:\n",
    "* **Mean squared error (MSE):** This metric takes the average of the squared differences between the predicted and actual values and penalizes large errors heavily. \n",
    "    * If the dataset contains outliers, they will have a disproportionate impact :( .\n",
    "\n",
    "* **Root mean squared error (RMSE):** This metric takes the square root of the mean squared error. \n",
    "    * It is also sensitive to outliers.\n",
    "\n",
    "* **Mean absolute error (MAE):** This metric is the average of the absolute differences between the predicted and actual values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Classification\n",
    "Classification is another version of supervised learning that focuses on predicting categorical labels or classes for a given set of input features. In a classification task, the algorithm learns to differentiate between different categories based on patterns in the training data. The output of a classification model is a discrete label representing the predicted class to which the input data belongs.\n",
    "\n",
    "Common examples of classification problems include email spam detection (binary classification), handwritten digit recognition (multiclass classification), and sentiment analysis (multiclass classification).\n",
    "\n",
    "Some common classification algorithms include:\n",
    "* **Logistic regression**\n",
    "* **Decision trees**\n",
    "* **Random forest**\n",
    "* **Support vector machines (SVM)**\n",
    "* **Naive Bayes**\n",
    "* **Extreme Gradient Boosting (XGBoost)**\n",
    "* **Neural networks**\n",
    "* **Linear discriminant analysis (LDA)**\n",
    "* **K-nearest neighbors (KNN)**\n",
    "\n",
    "Similar to regression, if you know that your problem involves predicting a categorical variable, you\n",
    "have the following model performance evaluation metrics available to you:\n",
    "\n",
    "* **Accuracy**: This metric measures the percentage of predictions that are correct.\n",
    "\n",
    "* **Precision**: Precision is the percentage of predicted positive classes that are positive. This can be an important metric if your dataset is imbalanced.\n",
    "\n",
    "* **Recall** (that is, the sensitivity or true positive rate): This metric is the percentage of actual positives that are correctly predicted as positive and is complementary to precision.\n",
    "\n",
    "* **Specificity** (that is, the true negative rate): Specificity is the percentage of actual negatives that are correctly predicted as negative. This is important in cases where false positives are costly, such as in medical diagnosis.\n",
    "\n",
    "* **F1 score**: This metric combines both precision and recall into one metric, and is a good compromise between both.\n",
    "\n",
    "* **Area under the receiver operating characteristic (AUC)**: This metric is a measure of how well the model can distinguish between positive and negative classes. The AUC is not affected by class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Unsupervised Learning\n",
    "\n",
    "Unsupervised machine learning is a fascinating branch of artificial intelligence that focuses on discovering patterns, relationships, and structure in data without explicit guidance from labeled outcomes. Unlike supervised learning, where models are trained on labeled data to make predictions, unsupervised learning aims to explore the inherent information in the data itself. This type of learning is particularly valuable for uncovering hidden insights, finding clusters, reducing dimensionality, and revealing underlying representations.\n",
    "\n",
    "**Clustering** and **dimensionality reduction** are two common use cases for unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Clustering\n",
    "Clustering is a type of unsupervised learning that **groups similar data points together based on their features**. The goal of clustering is to identify patterns in the data and group similar data points into clusters. Clustering algorithms are used in a variety of applications, such as customer segmentation, anomaly detection, and image segmentation.\n",
    "\n",
    "Some common clustering algorithms include:\n",
    "* **K-means**\n",
    "* **Hierarchical clustering**\n",
    "* **Density-based spatial clustering of applications with noise, DBSCAN**\n",
    "* **Spectral clustering**\n",
    "* **Ordering Points To Identify Clustering Structure (OPTICS)**\n",
    "* **Fuzzy c-means (FCM)**\n",
    "\n",
    "Some common methods for evaluating clustering algorithms are:\n",
    "* **Silhouette score**: quantifies how similar an object is to its cluster (cohesion) compared to other clusters (separation). It ranges from -1 to 1, where higher values indicate better-defined clusters.\n",
    "\n",
    "* **Elbow method**:  heuristic or graphical method used in determining the number of clusters in a data set for k-means.\n",
    "\n",
    "* **Adjusted Rand Index (ARI)**: evaluates the similarity between two clusterings of data, while accounting for the possibility of chance clustering. It ranges from -1 to 1, where higher values indicate better clustering quality.\n",
    "\n",
    "* **Normalized Mutual Information (NMI)**: uantifies the amount of information that’s shared between true class assignments and predicted clusters, normalized to account for cluster size. It ranges from 0 (no mutual information) and 1 (perfect correlation).\n",
    "\n",
    "* **Davies-Bouldin Index (DBI)**: It's calculated by averaging the maximum ratio of within-cluster distance to between-cluster distance for each cluster. A lower DBI indicates better clustering quality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Reinforcement Learning\n",
    "\n",
    "Clustering is a type of unsupervised learning that groups similar data points together based on their features. The goal of clustering is to identify patterns in the data and group similar data points into clusters. Clustering algorithms are used in a variety of applications, such as customer segmentation, anomaly detection, and image segmentation.\n",
    "\n",
    "Key Concepts of Reinforcement Learning:\n",
    "* **Agent**: The learner or decision-maker.\n",
    "* **Environment:** Everything the agent interacts with.\n",
    "* **State:** A specific situation in which the agent finds itself.\n",
    "* **Action**: All possible moves the agent can make.\n",
    "* **Reward**: Feedback from the environment based on the action taken.\n",
    "\n",
    "We will talk more about reinforcement learning in future notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. The bias-variance tradeoff\n",
    "\n",
    "**Model evaluation** is a crucial step in the machine learning pipeline that validates the utility of a model in real-world scenarios. It measures how well the model's predictions match actual outcomes, ensuring that the model can make accurate and reliable decisions beyond the training data. When evaluating a model's performance, it's important to consider two key aspects: bias and variance.\n",
    "\n",
    "**Bias** refers to the error due to overly simplistic assumptions in the learning algorithm, leading to an **underfit model** that misses relevant relationships. \n",
    "\n",
    "On the other hand, **variance** arises when a model is excessively complex and captures noise in the training data, resulting in an **overfit model** that doesn’t generalize well to new data.\n",
    "\n",
    "Striking the right **balance between bias and variance is a delicate challenge**. \n",
    "* High model complexity --> Low bias but high variace \n",
    "* Low complexity --> Lower variance but may increase bias. \n",
    "\n",
    "Achieving an optimal trade-off between bias and variance is crucial to developing models that can perform well on both training and test data.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:978/1*CgIdnlB6JK8orFKPXpc7Rg.png\" width=\"300\">\n",
    "\n",
    "**Model complexity** refers to the complexity and flexibility of a machine learning model in capturing relationships within the data. A more complex model can fit the training data more closely, potentially capturing intricate patterns and noise. However, this **increased complexity can also lead to** **overfitting**, where the model becomes highly tailored to the training data and struggles to generalize to new, unseen data. On the other hand, a **less complex model** may not capture all the nuances of the data, **leading to underfitting** , where it fails to capture even the basic relationships present in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Hyperparameter tuning methods\n",
    "\n",
    "Hyperparameters are the configuration settings that control the learning process of a machine learning model. They are different from model parameters, which are learned during training.\n",
    "\n",
    "Hyperparameters are set before the learning process begins and can have a significant impact on model performance.\n",
    "\n",
    "Hyperparameter tuning is the process of systematically finding and selecting optimal values for the hyperparameters of a machine learning model. Unlike model parameters, which are learned from data during training, hyperparameters are determined by the practitioner and define characteristics such as model complexity, learning rate, regularization strength, and more. \n",
    "\n",
    "The goal of hyperparameter tuning is to identify the hyperparameter values that lead to the best possible model performance on unseen data.\n",
    "\n",
    "Common hyperparameter tuning methods include:\n",
    "\n",
    "* **Grid Search** is a systematic approach to hyperparameter tuning. It involves defining a grid of possible hyperparameter values and exhaustively searching through all combinations. Grid search evaluates each combination using a predefined evaluation metric and identifies the configuration that yields the best performance. It guarantees thorough exploration but can be very computationally expensive.\n",
    "\n",
    "<img src=\"https://www.researchgate.net/publication/341691661/figure/fig2/AS:896464364507139@1590745168758/Comparison-between-a-grid-search-and-b-random-search-for-hyper-parameter-tuning-The.png\" width=\"600\">\n",
    "\n",
    "* **Random Search** takes a different approach by randomly sampling hyperparameter combinations from predefined ranges. This stochastic method explores a broader range of hyperparameter values in fewer iterations compared to grid search. While it might not guarantee exhaustive coverage, random search has shown to be effective in discovering good hyperparameter configurations with\n",
    "less computational cost.\n",
    "\n",
    "* **Bayesian Optimization** Bayesian optimization leverages probabilistic models to efficiently navigate the hyperparameter space. It uses the information gained from previous evaluations to guide the selection of subsequent hyperparameter combinations. Bayesian optimization strikes a balance between exploration (trying new combinations) and exploitation (focusing on promising areas), making it highly efficient for hyperparameter tuning.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*lu62RCEko0VYe-YZ\" width=\"700\">\n",
    "\n",
    "We will cover the implementation of these methods in the next chapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
