{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Linear regression\n",
    "\n",
    "*Thanks to [geekforgeeks](https://www.geeksforgeeks.org/ml-linear-regression/#)*.\n",
    "\n",
    "Linear regression is a type of supervised machine learning algorithm that calculates the linear relationship between the dependent variable and one or more independent features by fitting a linear equation to observed data.\n",
    "\n",
    "In addition to being a predictive tool, linear regression is the basis for several advanced models. Techniques such as regularization and support vector machines draw inspiration from linear regression and extend its utility.\n",
    "\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:1400/0*0plB5_YzkViGnD3j.gif\" width=\"400\">\n",
    "\n",
    "**Note 1**: \n",
    "* **Independent features** are also called predictors, inputs, or regressors.\n",
    "* The **dependent variable** is also called the target, output, or response.\n",
    "\n",
    "**Note 2**:\n",
    "* **Linear regression** is for **regression problems**, where the output is a **continuous value**.\n",
    "* For classification problems, where the **output is a discrete value**, **logistic regression** is used.\n",
    "\n",
    "## Overview\n",
    "1. How it works\n",
    "    * 1.1. What is the best fit line?\n",
    "    * 1.2. Cost function for linear regression\n",
    "    * 1.3. Gradient descent\n",
    "2. Assumptions\n",
    "4. Common pitfalls\n",
    "5. Regularization regression (linear regression extended)\n",
    "6. Metrics\n",
    "7. Implementation example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How it works\n",
    "Linear regression works by fitting a linear equation to the observed data.\n",
    "\n",
    "There are two types of linear regression:\n",
    "\n",
    "1. **Simple linear regression**: \n",
    "This is the simplest form of linear regression, and it involves only one independent variable and one dependent variable. The equation for simple linear regression is:\n",
    "    \n",
    "    $$y = b_0 + b_1x$$\n",
    "    \n",
    "    where:\n",
    "    * $y$ is the dependent variable.\n",
    "    * $b_0$ is the intercept or constant.\n",
    "    * $b_1$ is the coefficient or slope.\n",
    "    * $x$ is the independent feature.\n",
    "\n",
    "2. **Multiple linear regression**:\n",
    "\n",
    "This involves more than one independent variable and one dependent variable. The equation for multiple linear regression is:\n",
    "\n",
    "$$y = b_0 + b_1x_1 + b_2x_2 + ... + b_nx_n$$\n",
    "\n",
    "where:\n",
    "* $y$ is the dependent variable.\n",
    "* $b_0$ is the intercept or constant.\n",
    "* $b_1, b_2, ..., b_n$ are the coefficients.\n",
    "* $x_1, x_2, ..., x_n$ are the independent features.\n",
    "\n",
    "\n",
    "The goal of the algorithm is to find the best Fit Line equation that can predict the values based on the independent variables, which requires finding the optimal values for the coefficients $b_0, b_1, b_2, ..., b_n$.\n",
    "\n",
    "The coefficients are estimated using the least squares method, which minimizes the sum of the squared differences between the observed values and the predicted values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. What is the best fit line?\n",
    "\n",
    "Our primary objective while using linear regression is to locate the best-fit line, which implies that the error between the predicted and actual values should be kept to a minimum. \n",
    "\n",
    "The best-fit line is the line that minimizes this error.\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20231129130431/11111111.png\" width=\"400\">\n",
    "\n",
    "Linear regression performs the task to predict a dependent variable value (y) based on a given independent variable (x).\n",
    "\n",
    "We utilize the cost function to compute the best values in order to get the best fit line since different values for weights or the coefficient of lines result in different regression lines.\n",
    "\n",
    "We search for the best values for the coefficients that minimizes the error between the predicted y value (pred) and the true y value (y). \n",
    "\n",
    "$$\\text{minimize} [\\frac{1}{n} \\sum_{i=1}^{n} (\\text{pred}_i - y_i)^2]$$\n",
    "\n",
    "Residuals are the differences between the true value and the predicted value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Cost function for linear regression\n",
    "The cost function or the loss function is nothing but the error or difference between the predicted value *pred* and the true value *y* is minimum.\n",
    "\n",
    "The cost function for linear regression is the mean squared error (MSE), which calculates the average of the squared errors between the predicted values *pred_i * and the actual values *y_i*.\n",
    "\n",
    "MSE function can be calculated as:\n",
    "\n",
    "$$Cost function(J) = \\frac{1}{n} \\sum_{i=1}^{n} (\\text{pred}_i - y_i)^2$$\n",
    "\n",
    "Utilizing the MSE function, the iterative process of gradient descent is applied to update the coefficients (b_n). This ensures that the MSE value converges to the global minima, signifying the most accurate fit of the linear regression line to the dataset.\n",
    "\n",
    "### 1.3. Gradient Descent\n",
    "A linear regression model can be trained using the optimization algorithm gradient descent by iteratively modifying the model’s parameters to reduce the mean squared error (MSE) of the model on a training dataset. \n",
    "To update b_n values in order to reduce the Cost function (minimizing RMSE value) and achieve the best-fit line the model uses Gradient Descent.\n",
    "\n",
    "A gradient is nothing but a derivative that defines the effects on outputs of the function with a little bit of variation in inputs.\n",
    "\n",
    "$$J'(b_0) = \\frac{dJ(b_0,b_1)}{db_0} = \\frac{d}{db_0}[\\frac{1}{n} \\sum_{i=1}^{n} (\\text{pred}_i - y_i)^2] = ... = \\frac{2}{n} \\sum_{i=1}^{n} (\\text{pred}_i - y_i)$$\n",
    "\n",
    "$$J'(b_1) = \\frac{dJ(b_0,b_1)}{db_1} = \\frac{d}{db_1}[\\frac{1}{n} \\sum_{i=1}^{n} (\\text{pred}_i - y_i)^2] = ... = \\frac{2}{n} \\sum_{i=1}^{n} (\\text{pred}_i - y_i) * x_i$$\n",
    "\n",
    "Finding the coefficients of a linear equation that best fits the training data is the objective of linear regression. By moving in the direction of the Mean Squared Error negative gradient with respect to the coefficients, the coefficients can be changed. And the respective intercept and coefficient of X will be if α is the learning rate.\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20230424151248/Gradient-Descent-for-ML-Linear-Regression-(1).webp\" width=\"400\">\n",
    "\n",
    "$$b_0 = b_0 - \\alpha * J'(b_0) = b_0 - \\alpha *  (\\frac{2}{n} \\sum_{i=1}^{n} (\\text{pred}_i - y_i))$$\n",
    "$$b_1 = b_1 - \\alpha * J'(b_1) = b_1 - \\alpha * ( \\frac{2}{n} \\sum_{i=1}^{n} (\\text{pred}_i - y_i) * x_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assumptions\n",
    "\n",
    "### 2.1. Simple Linear Regression\n",
    "\n",
    "* **Linearity**: The relationship between the dependent variable and independent features is linear.\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20231123113044/python-linear-regression-4.png\" width=\"700\">\n",
    "\n",
    "* **Independence**: The independent features are not correlated with each other.\n",
    "\n",
    "* **Homoscedasticity**: When a dataset exhibits homoscedasticity, it implies that the variance of the errors is the same for all values of the predictor variable(s). The opposite of homoscedasticity is heteroscedasticity, which occurs when the variability of the errors (or residuals) or data points changes systematically as you move along the range of the independent variable(s).\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20231123113103/python-linear-regression-5.png\" width=\"400\">\n",
    "\n",
    "* **Normality**: The residuals (the differences between observed and predicted values)\n",
    "should follow a normal distribution.\n",
    "\n",
    "### 2.2. Multiple Linear Regression\n",
    "For Multiple Linear Regression, all four of the assumptions from Simple Linear Regression apply. In addition to this, below are few more:\n",
    "* **No multicollinearity**: The independent features are not correlated with each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. **Common pitfalls** or challenges in linear regression\n",
    "While linear regression is a valuable tool, there are several pitfalls to watch out for:\n",
    "\n",
    "* **Violating assumptions:** Failing to meet the assumptions of linear regression can lead to inaccurate results and misleading interpretations.\n",
    "\n",
    "*  **Outliers**: Outliers can disproportionately influence the model’s coefficients, leading to an\n",
    "erroneous fit.\n",
    "\n",
    "* **Multicollinearity**: Multicollinearity is when independent variables are highly correlated. When\n",
    "this happens, it can be difficult to discern their individual effects on the dependent variable.\n",
    "\n",
    "*  **Overfitting**: Adding too many variables or polynomial terms can result in overfitting, where\n",
    "the model captures noise in the data rather than the underlying pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regularization regression (linear regression extended)\n",
    "\n",
    "Regularized regression (specifically L1 (lasso) and L2 (ridge) regression) is an extension of linear regression that addresses some of its limitations. While linear regression aims to find the best-fitting\n",
    "line or hyperplane, regularized regression introduces penalty terms to prevent overfitting and improve model performance.\n",
    "\n",
    "Regularization adds a penalty term to the linear regression objective function, which discourages the model from assigning excessively large coefficients to the features. L1 regularization adds the absolute values of the coefficients as penalties, leading to some coefficients being exactly zero, while L2 regularization adds the squared values of the coefficients, which enforces smaller but non-zero coefficients:\n",
    "\n",
    "L1 regularization:\n",
    "$$minimize [ \\sum_{i=1}^{n}(y_i - b_0 - \\sum_{j=1}^{p} b_jx_{ij})^2 + \\lambda \\sum_{j=1}^{p} |b_j|]$$\n",
    "\n",
    "L2 regularization:\n",
    "$$minimize [ \\sum_{i=1}^{n}(y_i - b_0 - \\sum_{j=1}^{p} b_jx_{ij})^2 + \\lambda \\sum_{j=1}^{p} b_j^2]$$\n",
    "\n",
    "L1 and L2 regularization are particularly useful in the following ways:\n",
    "* **Feature selection:** L1 regularization can force some coefficients to be exactly zero, effectively\n",
    "performing feature selection and identifying the most important variables.\n",
    "* **Multicollinearity management**: L2 regularization helps mitigate the effects of multicollinearity\n",
    "by shrinking the coefficients towards zero.\n",
    "* **High-dimensional data:** Regularized regression is valuable when dealing with datasets with a large number of features, preventing overfitting and improving generalization.\n",
    "\n",
    "**Elatic Net** is a combination of L1 and L2 regularization, which combines the benefits of both methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metrics\n",
    "A variety of evaluation measures can be used to determine the strength of any linear regression model. Some of the most common metrics include:\n",
    "\n",
    "* **Mean Squared Error (MSE)**: The average of the squared differences between the predicted and actual values. MSE is sensitive to outliers, as it squares the errors.\n",
    "\n",
    "$$MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\text{pred}_i)^2$$\n",
    "\n",
    "where:\n",
    "* $n$ is the number of observations or data points.\n",
    "* $y_i$ is the actual value of the ith data point.\n",
    "* $\\text{pred}_i$ is the predicted value of the ith data point.\n",
    "\n",
    "__\n",
    "\n",
    "* **Mean Absolute Error (MAE)**: The average of the absolute differences between the predicted and actual values. MAE is less sensitive to outliers than MSE and is usually used to calculate the accuracy of the regression model. Lower MAE value indicates better model performance. It is not as sensitive to the outliers as we consider absolute differences.\n",
    "\n",
    "$$MAE = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\text{pred}_i|$$\n",
    "\n",
    "* **Root Mean Squared Error (RMSE)**: The square root of the average of the squared differences between the predicted and actual values. RMSE is sensitive to outliers, as it squares the errors. RSME is not as good of a metric as R-squared. Root Mean Squared Error can fluctuate when the units of the variables vary since its value is dependent on the variables’ units (it is not a normalized measure).\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\text{pred}_i)^2}$$\n",
    "\n",
    "* **Coefficient of Determination (R-squared, R2)**: R-Squared is a statistic that indicates how much variation the developed model can explain or capture. R2 ranges from 0 to 1, with higher values indicating a better fit. R squared metric is a measure of the proportion of variance in the dependent variable that is explained the independent variables in the model.\n",
    "\n",
    "$$R^2 = 1 - \\frac{RSS}{TSS} = 1 - \\frac{\\sum_{i=2}^{n} (y_i - b_0 - b_1x_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}$$\n",
    "\n",
    "where:\n",
    "* $RSS$ is the residual sum of squares.\n",
    "* $TSS$ is the total sum of squares.\n",
    "* $y_i$ is the actual value of the ith data point.\n",
    "* $\\text{pred}_i$ is the predicted value of the ith data point.\n",
    "* $\\bar{y}$ is the mean of the actual values.\n",
    "\n",
    "__\n",
    "\n",
    "* **Adjusted R-squared**: measures the proportion of variance in the dependent variable that is explained by independent variables in a regression model. Adjusted R-square accounts the number of predictors in the model and penalizes the model for including irrelevant predictors that don’t contribute significantly to explain the variance in the dependent variables. It is useful when comparing models with different numbers of predictors and helps prevent overfitting.\n",
    "\n",
    "$$Adjusted R^2 = 1 - \\frac{(1 - R^2)*(n - 1)}{n - k - 1}$$\n",
    "\n",
    "where:\n",
    "* $n$ is the number of observations.\n",
    "* $k$ is the number of predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Implementation example\n",
    "\n",
    "Example using sklear implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.56\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load the dataset\n",
    "housing = fetch_california_housing()\n",
    "X = housing.data\n",
    "y = housing.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model and compute error\n",
    "linear_reg = LinearRegression()\n",
    "linear_reg.fit(X_train, y_train)\n",
    "y_pred = linear_reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also see this example but explicitly writing our functions for the linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.axes as ax\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "# Sample data\n",
    "hours_studied = np.array([2, 4, 6, 8, 10, 12, 1])\n",
    "scores = np.array([50, 65, 75, 80, 95, 98, 30])\n",
    "\n",
    "X_train = hours_studied\n",
    "y_train = scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression:\n",
    "    def __init__(self):\n",
    "        self.parameters = {}\n",
    "\n",
    "    def forward_propagation(self, train_input):\n",
    "        \"Calculate the predictions using the formula y = m*x +c\"\n",
    "        m = self.parameters['m']\n",
    "        c = self.parameters['c']\n",
    "        predictions = np.multiply(m, train_input) + c\n",
    "        return predictions \n",
    "\n",
    "    def cost_function(self, predictions, train_output):\n",
    "        \"Compute the MSE between the predicted values and actual values.\"\n",
    "        cost = np.mean((train_output - predictions) ** 2)  # Mean Square Error\n",
    "        return cost \n",
    "\n",
    "    def backward_propagation(self, train_input, train_output, predictions):\n",
    "        \"Calculate the gradients for m and c using partial derivatives of the cost function.\"\n",
    "        df = (predictions-train_output)\n",
    "        # dm= 2/n * mean of (predictions-actual) * input\n",
    "        dm = 2 * np.mean(np.multiply(train_input, df))  # deriv. with respect to m\n",
    "        # dc = 2/n * mean of (predictions-actual)\n",
    "        dc = 2 * np.mean(df)  # deriv. with respect to c\n",
    "        derivatives = {'dm': dm, 'dc': dc} \n",
    "        return derivatives \n",
    "\n",
    "    def update_parameters(self, derivatives, learning_rate):\n",
    "        \"Update m & c by moving in the direction opposite to the gradient, scaled by the learning rate.\"\n",
    "        self.parameters['m'] -= learning_rate * derivatives['dm']\n",
    "        self.parameters['c'] -= learning_rate * derivatives['dc']\n",
    "\n",
    "    def train(self, train_input, train_output, learning_rate, iters):\n",
    "        # Initialize random parameters\n",
    "        self.parameters['m'] = np.random.uniform(0, 1) * -1\n",
    "        self.parameters['c'] = np.random.uniform(0, 1) * -1\n",
    "\n",
    "        # Initialize loss\n",
    "        self.loss = []\n",
    "\n",
    "        for i in range(iters):\n",
    "            # Forward propagation\n",
    "            predictions = self.forward_propagation(train_input)\n",
    "\n",
    "            # Cost function\n",
    "            cost = self.cost_function(predictions, train_output)\n",
    "\n",
    "            # Backward propagation\n",
    "            derivatives = self.backward_propagation(train_input, train_output, predictions)\n",
    "\n",
    "            # Update parameters\n",
    "            self.update_parameters(derivatives, learning_rate)\n",
    "\n",
    "            # Append loss and print\n",
    "            self.loss.append(cost)\n",
    "            print(f\"Iteration = {i + 1}, Loss = {cost}\")\n",
    "\n",
    "        return self.parameters, self.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 421.33\n"
     ]
    }
   ],
   "source": [
    "#Example usage\n",
    "linear_reg = LinearRegression()\n",
    "parameters, loss = linear_reg.train(X_train, y_train, 0.0001, 200)\n",
    "\n",
    "y_pred = parameters['m'] * hours_studied + parameters['c']\n",
    "mse = mean_squared_error(y_train, y_pred)\n",
    "print(f'Mean Squared Error: {mse:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x203e3da2d00>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots() \n",
    "x_vals = np.linspace(min(hours_studied), max(hours_studied), 100) \n",
    "line, = ax.plot(x_vals, parameters['m'] * x_vals +\n",
    "                parameters['c'], color='red', label='Regression Line') \n",
    "ax.scatter(hours_studied, y_train, marker='o', color='green', label='Training Data') "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
