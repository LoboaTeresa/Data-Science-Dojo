{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Random Forest\n",
    "\n",
    "The Random forest or Random Decision Forest is a **supervised Machine Learning algorithm** used for both **classification** and **regression** tasks. It is an **extension of decision tree algorithms** that addresses their limitations by **combining multiple trees** to create a more robust and accurate predictive model. \n",
    "\n",
    "Random forest is known for its ability to handle complex relationships, reduce overfitting, and provide insight into the importance of features.\n",
    "\n",
    "## Overview\n",
    "1. How it works\n",
    "    * 1.1. What is a decision tree?\n",
    "    * 1.2. What are ensemble methods?\n",
    "    * 1.3. What is Bagging and Boosting?\n",
    "    * 1.4. Steps in Random Forest\n",
    "    * 1.5. Advantages\n",
    "    * 1.6 Hyperparameters\n",
    "2. Assumptions\n",
    "3. Common pitfalls\n",
    "4. Implementation examples\n",
    "    * 4.1 Classification\n",
    "    * 4.2 Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. How it works\n",
    "Random forest builds an ensemble of decision trees, each trained on a different subset of the data and considering a subset of features. \n",
    "\n",
    "**Ensemble methods** are machine learning techniques that **combine the predictions of multiple individual models** to produce a more robust overall prediction. The idea behind ensemble methods is to use the diversity of different models to improve the overall accuracy, stability, and generalization of the predictive model.\n",
    "\n",
    "<img src=\"https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEiN2KPoea9rFZo4nb0SZKrBrEUjNv-xaqB7gF6Htl5lY5AtOmKH1yFalD9Y6XHNNgtUYqsJCPUr-7a4MJIvdcubXogxerrskVqKfQGhKSpUyrnroLhEi6P5vMXqYE22J3_dnLRuWiBv5Nw/s0/Random+Forest+03.gif\" width=\"800px\">\n",
    "\n",
    "### 1.1. **What is a decision tree?**\n",
    "Decision trees are a popular and powerful tool used in various fields such as machine learning, data mining, and statistics. They provide a clear and intuitive way to make decisions based on data by modeling the relationships between different variables.\n",
    "\n",
    "### 1.2. **What are ensemble methods?**\n",
    "Ensemble learning models work just like a **group of diverse experts teaming up to make decisions**. In ensemble learning, different models, often of the same type or different types, team up to enhance predictive performance.\n",
    "\n",
    "Some popular ensemble models include- **XGBoost**, AdaBoost, LightGBM, **Random Forest**, Bagging, Voting etc.\n",
    "\n",
    "\n",
    "### 1.3. **What is Bagging and Boosting?**\n",
    "* **Bagging** is an ensemble learning model, where **multiple weak models are trained on different subsets** of the training data and prediction is made by **averaging the prediction** of the weak models for regression problems and considering the **majority vote** for classification problems.\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20210707140912/Bagging.png\" width=\"800px\">\n",
    "\n",
    "* **Boosting** is an ensemble learning model, where **multiple weak models are trained sequentially**.  In this method, **each model tries to correct the errors made by the previous models**. Each model is trained on a modified version of the dataset, the instances that were misclassified by the previous models are given more weight. The final prediction is made by **weighted voting**.\n",
    "\n",
    "<img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20210707140911/Boosting.png\" width=\"800px\">\n",
    "\n",
    "### 1.4. **Steps in Random Forest**\n",
    "1. Select random K data points from the training set.\n",
    "2. Build the decision trees associated with the selected data points(Subsets).\n",
    "3. Choose the number N for decision trees that you want to build.\n",
    "4. Repeat Step 1 and 2.\n",
    "5. For new data points, find the predictions of each decision tree, and assign the new data points to the category that wins the majority votes.\n",
    "\n",
    "### 1.5. **Advantages**\n",
    "Random forest offers several **advantages over individual decision trees**:\n",
    "* **Reduced overfitting:** By averaging predictions from multiple trees, random forest mitigates the risk of overfitting and provides better generalization.\n",
    "\n",
    "* **Robustness**: Random forest is less sensitive to noisy data and outliers compared to a single decision tree.\n",
    "\n",
    "* **Non-linearity handling:** It can capture complex nonlinear relationships between features and the target variable.\n",
    "\n",
    "* **Feature importance:** Random forest quantifies the importance of each feature, aiding in feature selection and interpretation.\n",
    "\n",
    "Random forest **calculates feature importance** based on how much a particular feature contributes to the overall predictive performance of the ensemble. The importance of a feature is assessed by measuring the decrease in a specific metric when the values of that feature are randomly permuted while keeping the other features constant.\n",
    "\n",
    "### 1.6. **Hyperparameters**\n",
    "Random forests have several **hyperparameters** that allow you to customize and fine-tune the behavior of the ensemble algorithm:\n",
    "\n",
    "* **n_estimators**: The number of decision trees in the ensemble (forest). Increasing the number of trees generally improves performance until reaching a point of diminishing returns or overfitting.\n",
    "\n",
    "* **max_depth**: The maximum depth of each decision tree. Limits the number of splits. Deeper trees can capture more complex relationships but are more prone to overfitting.\n",
    "\n",
    "* **min_samples_split**: The minimum number of samples required to split an node further. It prevents nodes with very few samples from being split, potentially reducing noise.\n",
    "\n",
    "* and [other hyperparameters](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) like `min_samples_leaf`, `max_features`, `criterion`, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assumptions\n",
    "Random forest is a powerful ensemble learning algorithm that combines multiple decision trees to make predictions. \n",
    "\n",
    "Unlike some other machine learning algorithms, random forest has fewer assumptions.\n",
    "\n",
    "However, it’s important to note that while individual decision trees have certain assumptions, the ensemble method helps to mitigate the impact of these assumptions:\n",
    "* **Independence of features**: Decision trees assume that features are independent of each other. However, this assumption is less critical in random forests because each tree is trained on a different subset of features.\n",
    "\n",
    "* **Linear relationships**: Decision trees assume that the relationships between features and the target variable are linear. Random forest, being an ensemble of decision trees, can capture both linear and nonlinear relationships in the data due to the diversity of trees it comprises.\n",
    "\n",
    "* **Homoscedasticity**: Decision trees do not make explicit assumptions about the homoscedasticity (constant variance) of errors. Similarly, random forest, being a combination of decision trees, is not directly affected by this assumption.\n",
    "\n",
    "* **Normality of residuals**: Decision trees do not rely on the assumption of normality of residuals, and the random forest algorithm inherits this flexibility. However, if you’re using random forest as part of a broader analysis that assumes normality (for example, hypothesis testing), you should consider this aspect in your overall approach.\n",
    "\n",
    "* **Feature scaling**: Random forest is relatively insensitive to the scale of features. It doesn’t require features to be standardized or normalized, unlike some other algorithms, such as gradient\n",
    "boosting or K-means clustering.\n",
    "\n",
    "* **Multicollinearity**: Random forest can handle multicollinearity (high correlation between features) because it selects a random subset of features at each split. This helps to reduce the impact of correlated features on the model’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Common pitfalls\n",
    "While random forest is a powerful algorithm, it’s important to be aware of potential pitfalls:\n",
    "\n",
    "* **Overfitting with too many trees:** Although random forest reduces overfitting, using an excessive number of trees can still lead to unnecessary computational complexity.\n",
    "\n",
    "* **Bias toward dominant classes:** In imbalanced datasets, random forest might favor the majority class due to its inherent averaging mechanism.\n",
    "\n",
    "* **Computation and memory:** Training a large random forest can be computationally expensive and memory-intensive.\n",
    "\n",
    "* **Feature selection:** While random forest provides feature importance, it might not always identify the optimal subset of features for a specific problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementation examples\n",
    "Let’s see how to implement random forest for classification and regression tasks using Python’s scikit-learn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, r2_score\n",
    "\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Main data directory\n",
    "mainpath=\"../data/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>survived</th>\n",
       "      <th>name</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>ticket</th>\n",
       "      <th>fare</th>\n",
       "      <th>cabin</th>\n",
       "      <th>embarked</th>\n",
       "      <th>boat</th>\n",
       "      <th>body</th>\n",
       "      <th>home.dest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allen, Miss. Elisabeth Walton</td>\n",
       "      <td>female</td>\n",
       "      <td>29.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24160</td>\n",
       "      <td>211.3375</td>\n",
       "      <td>B5</td>\n",
       "      <td>S</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>St Louis, MO</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Allison, Master. Hudson Trevor</td>\n",
       "      <td>male</td>\n",
       "      <td>0.9167</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Miss. Helen Loraine</td>\n",
       "      <td>female</td>\n",
       "      <td>2.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Mr. Hudson Joshua Creighton</td>\n",
       "      <td>male</td>\n",
       "      <td>30.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>135.0</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Allison, Mrs. Hudson J C (Bessie Waldo Daniels)</td>\n",
       "      <td>female</td>\n",
       "      <td>25.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>113781</td>\n",
       "      <td>151.5500</td>\n",
       "      <td>C22 C26</td>\n",
       "      <td>S</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Montreal, PQ / Chesterville, ON</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pclass  survived                                             name     sex  \\\n",
       "0       1         1                    Allen, Miss. Elisabeth Walton  female   \n",
       "1       1         1                   Allison, Master. Hudson Trevor    male   \n",
       "2       1         0                     Allison, Miss. Helen Loraine  female   \n",
       "3       1         0             Allison, Mr. Hudson Joshua Creighton    male   \n",
       "4       1         0  Allison, Mrs. Hudson J C (Bessie Waldo Daniels)  female   \n",
       "\n",
       "       age  sibsp  parch  ticket      fare    cabin embarked boat   body  \\\n",
       "0  29.0000      0      0   24160  211.3375       B5        S    2    NaN   \n",
       "1   0.9167      1      2  113781  151.5500  C22 C26        S   11    NaN   \n",
       "2   2.0000      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
       "3  30.0000      1      2  113781  151.5500  C22 C26        S  NaN  135.0   \n",
       "4  25.0000      1      2  113781  151.5500  C22 C26        S  NaN    NaN   \n",
       "\n",
       "                         home.dest  \n",
       "0                     St Louis, MO  \n",
       "1  Montreal, PQ / Chesterville, ON  \n",
       "2  Montreal, PQ / Chesterville, ON  \n",
       "3  Montreal, PQ / Chesterville, ON  \n",
       "4  Montreal, PQ / Chesterville, ON  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = \"titanic/titanic3.csv\"\n",
    "fullpath = os.path.join(mainpath, filepath)\n",
    "data = pd.read_csv(fullpath)  # Load the data\n",
    "\n",
    "data.head() # Show the first 5 rows of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1309 entries, 0 to 1308\n",
      "Data columns (total 14 columns):\n",
      " #   Column     Non-Null Count  Dtype  \n",
      "---  ------     --------------  -----  \n",
      " 0   pclass     1309 non-null   int64  \n",
      " 1   survived   1309 non-null   int64  \n",
      " 2   name       1309 non-null   object \n",
      " 3   sex        1309 non-null   object \n",
      " 4   age        1046 non-null   float64\n",
      " 5   sibsp      1309 non-null   int64  \n",
      " 6   parch      1309 non-null   int64  \n",
      " 7   ticket     1309 non-null   object \n",
      " 8   fare       1308 non-null   float64\n",
      " 9   cabin      295 non-null    object \n",
      " 10  embarked   1307 non-null   object \n",
      " 11  boat       486 non-null    object \n",
      " 12  body       121 non-null    float64\n",
      " 13  home.dest  745 non-null    object \n",
      "dtypes: float64(3), int64(4), object(7)\n",
      "memory usage: 143.3+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing target values\n",
    "data = data.dropna(subset=['survived'])\n",
    "data = data.dropna(subset=['fare'])\n",
    "\n",
    "# Select relevant features and target variable\n",
    "X = data[['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare']]\n",
    "y = data['survived']\n",
    "\n",
    "# Convert categorical variable 'Sex' to numerical using .loc\n",
    "X.loc[:, 'sex'] = X['sex'].map({'female': 0, 'male': 1})\n",
    "\n",
    "# Handle missing values in the 'Age' column using .loc\n",
    "X.loc[:, 'age'].fillna(X['age'].median(), inplace=True)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.80\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.86      0.84       156\n",
      "           1       0.78      0.72      0.75       106\n",
      "\n",
      "    accuracy                           0.80       262\n",
      "   macro avg       0.80      0.79      0.79       262\n",
      "weighted avg       0.80      0.80      0.80       262\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a Random Forest Classifier\n",
    "rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the classifier\n",
    "rf_classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_classifier.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.26\n",
      "R-squared Score: 0.81\n"
     ]
    }
   ],
   "source": [
    "# Load the California Housing dataset\n",
    "california_housing = fetch_california_housing()\n",
    "california_data = pd.DataFrame(california_housing.data, columns=california_housing.feature_names)\n",
    "california_data['MEDV'] = california_housing.target\n",
    "\n",
    "# Select relevant features and target variable\n",
    "X = california_data.drop('MEDV', axis=1)\n",
    "y = california_data['MEDV']\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest Regressor\n",
    "rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the regressor\n",
    "rf_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = rf_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")  # the smaller the better\n",
    "print(f\"R-squared Score: {r2:.2f}\")  # how well the model fits the data (0-100%)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-training",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
